{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)  # Toutes les colonnes\n",
    "pd.set_option('display.width', None)        # Pas de limite de largeur pour les lignes\n",
    "pd.set_option('display.max_colwidth', None) # Pas de limite pour la largeur des cellules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def change_column_types(df):\n",
    "    \"\"\"\n",
    "    Change les types de plusieurs colonnes dans un DataFrame pandas.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Le DataFrame pandas.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un nouveau DataFrame avec les colonnes modifiées.\n",
    "    \"\"\"\n",
    "    # Dictionnaire des colonnes et leurs types cibles\n",
    "    dict_type = {\n",
    "        'id_mutation': 'string',\n",
    "        'date_mutation': 'datetime64[ns]',\n",
    "        'numero_disposition': 'string',\n",
    "        'nature_mutation': 'string',\n",
    "        'valeur_fonciere': 'float64',\n",
    "        'adresse_numero': 'string',\n",
    "        'adresse_suffixe': 'string',\n",
    "        'adresse_nom_voie': 'string',\n",
    "        'adresse_code_voie': 'string',\n",
    "        'code_postal': 'string',\n",
    "        'code_commune': 'string',\n",
    "        'nom_commune': 'string',\n",
    "        'code_departement': 'string',\n",
    "        'ancien_code_commune': 'string',\n",
    "        'ancien_nom_commune': 'string',\n",
    "        'id_parcelle': 'string',\n",
    "        'ancien_id_parcelle': 'string',\n",
    "        'numero_volume': 'string',\n",
    "        'lot1_numero': 'string',\n",
    "        'lot1_surface_carrez': 'float64',\n",
    "        'lot2_numero': 'string',\n",
    "        'lot2_surface_carrez': 'float64',\n",
    "        'lot3_numero': 'string',\n",
    "        'lot3_surface_carrez': 'float64',\n",
    "        'lot4_numero': 'string',\n",
    "        'lot4_surface_carrez': 'float64',\n",
    "        'lot5_numero': 'string',\n",
    "        'lot5_surface_carrez': 'float64',\n",
    "        'nombre_lots': 'float64',\n",
    "        'code_type_local': 'string',\n",
    "        'type_local': 'string',\n",
    "        'surface_reelle_bati': 'float64',\n",
    "        'nombre_pieces_principales': 'float64',\n",
    "        'code_nature_culture': 'string',\n",
    "        'nature_culture': 'string',\n",
    "        'code_nature_culture_speciale': 'string',\n",
    "        'nature_culture_speciale': 'string',\n",
    "        'surface_terrain': 'float64',\n",
    "        'longitude': 'float64',\n",
    "        'latitude': 'float64'\n",
    "    }\n",
    "\n",
    "    # Conversion des types de colonnes\n",
    "    for column_name, column_type in dict_type.items():\n",
    "        if column_name in df.columns:  # Vérifie que la colonne existe\n",
    "            try:\n",
    "                df[column_name] = df[column_name].astype(column_type)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la conversion de la colonne {column_name}: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def data_loader(path, departements=[], annees=[]):\n",
    "    \"\"\"\n",
    "    Charge les fichiers CSV d'un répertoire et les concatène dans un DataFrame pandas.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Chemin vers le dossier contenant les données.\n",
    "        departements (list): Liste des départements à inclure (ex: [1, 75]).\n",
    "        annees (list): Liste des années à inclure (ex: [2020, 2021]).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame pandas contenant toutes les données chargées.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Liste des années\n",
    "    annees_list = os.listdir(path) if not annees else [str(annee) for annee in annees]\n",
    "\n",
    "    for annee in annees_list:\n",
    "        cur_year = os.path.join(path, annee)\n",
    "\n",
    "        # Liste des départements\n",
    "        if not departements:\n",
    "            departements_list = os.listdir(cur_year)\n",
    "        else:\n",
    "            departements_list = [f\"{departement:02}.csv.gz\" for departement in departements]\n",
    "\n",
    "        for departement in departements_list:\n",
    "            file = os.path.join(cur_year, departement)\n",
    "            try:\n",
    "                # Charger les données\n",
    "                temp_df = pd.read_csv(file, low_memory=False)\n",
    "                temp_df = change_column_types(temp_df)\n",
    "                df = pd.concat([df, temp_df], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du chargement du fichier {file}: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data_dvf'\n",
    "df = data_loader(path,departements=[75,92,93,94,95,91,77,78])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    df[\"valeur_fonciere\"].notnull() &\n",
    "    df[\"longitude\"].notnull() &\n",
    "    df[\"latitude\"].notnull() &\n",
    "    (df[\"surface_reelle_bati\"].notna() | df[\"surface_terrain\"].notna())\n",
    "]\n",
    "\n",
    "# Sélectionner les colonnes nécessaires\n",
    "data = df[[\n",
    "    'date_mutation', 'type_local', 'surface_reelle_bati', 'nombre_lots',\n",
    "    'lot1_surface_carrez', 'lot2_surface_carrez', 'lot3_surface_carrez',\n",
    "    'lot4_surface_carrez', 'lot5_surface_carrez', 'nombre_pieces_principales',\n",
    "    'nature_culture', 'surface_terrain', 'longitude', 'latitude', 'valeur_fonciere'\n",
    "]]\n",
    "\n",
    "# Ajouter les colonnes calculées\n",
    "data = data.assign(\n",
    "    sin_month=np.sin(2 * np.pi * pd.to_datetime(data[\"date_mutation\"]).dt.month / 12),\n",
    "    cos_month=np.cos(2 * np.pi * pd.to_datetime(data[\"date_mutation\"]).dt.month / 12),\n",
    "    year=pd.to_datetime(data[\"date_mutation\"]).dt.year,\n",
    "    lot1_surface_carrez=data['lot1_surface_carrez'].fillna(0),\n",
    "    lot2_surface_carrez=data['lot2_surface_carrez'].fillna(0),\n",
    "    lot3_surface_carrez=data['lot3_surface_carrez'].fillna(0),\n",
    "    lot4_surface_carrez=data['lot4_surface_carrez'].fillna(0),\n",
    "    lot5_surface_carrez=data['lot5_surface_carrez'].fillna(0),\n",
    "    surface_reelle_bati=data['surface_reelle_bati'].fillna(0),\n",
    "    surface_terrain=data['surface_terrain'].fillna(0)\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la somme des surfaces Carrez pour créer la colonne \"total_surface_carrez\"\n",
    "data[\"total_surface_carrez\"] = (\n",
    "    data[\"lot1_surface_carrez\"] +\n",
    "    data[\"lot2_surface_carrez\"] +\n",
    "    data[\"lot3_surface_carrez\"] +\n",
    "    data[\"lot4_surface_carrez\"] +\n",
    "    data[\"lot5_surface_carrez\"]\n",
    ")\n",
    "\n",
    "# Si \"total_surface_carrez\" est 0, utiliser \"surface_reelle_bati\" à la place\n",
    "data[\"total_surface_carrez\"] = np.where(\n",
    "    data[\"total_surface_carrez\"] == 0,\n",
    "    data[\"surface_reelle_bati\"],\n",
    "    data[\"total_surface_carrez\"]\n",
    ")\n",
    "\n",
    "# Calculer \"valeur_fonciere_m2\" en fonction de \"surface_reelle_bati\" ou \"surface_terrain\"\n",
    "data[\"valeur_fonciere_m2\"] = np.where(\n",
    "    data[\"surface_reelle_bati\"] != 0,\n",
    "    (data[\"valeur_fonciere\"] / data[\"surface_reelle_bati\"]).round(0),\n",
    "    (data[\"valeur_fonciere\"] / data[\"surface_terrain\"]).round(0)\n",
    ")\n",
    "data = data[data['valeur_fonciere_m2'] != 0]\n",
    "data[\"valeur_fonciere_m2_log\"]= data[\"valeur_fonciere_m2\"].apply(lambda x: np.log10(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "x = data['valeur_fonciere_m2_log'].to_numpy()\n",
    "# Calcul de la moyenne et de l'écart-type\n",
    "mu, sigma = norm.fit(x)  # Ajuste une gaussienne et retourne la moyenne et l'écart-type\n",
    "\n",
    "# Tracer l'histogramme des données\n",
    "plt.hist(x, bins=100, density=True, alpha=0.6, color='skyblue', label=\"Données\")\n",
    "\n",
    "# Créer une courbe gaussienne avec les paramètres ajustés\n",
    "x = np.linspace(min(x), max(x), 1000)\n",
    "pdf = norm.pdf(x, mu, sigma)  # Fonction de densité de probabilité\n",
    "plt.plot(x, pdf, color='red', linewidth=2, label=f\"Fit Gaussien\\n$\\mu$={mu:.2f}, $\\sigma$={sigma:.2f}\")\n",
    "\n",
    "# Ajouter des labels et une légende\n",
    "plt.xlabel(\"Valeurs\")\n",
    "plt.ylabel(\"Densité\")\n",
    "plt.title(\"Ajustement Gaussien\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Moyenne (mu): {mu:.2f}\")\n",
    "print(f\"Écart-type (sigma): {sigma:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "model = IsolationForest(contamination=0.1)\n",
    "data['anomalie'] = model.fit_predict(data[['valeur_fonciere_m2']])\n",
    "df_cleaned = data[data['anomalie'] == 1]\n",
    "print(f\"{data.size-df_cleaned.size} lignes enlevées, taille finale : {df_cleaned.size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_cleaned['valeur_fonciere_m2'].to_numpy()\n",
    "# Calcul de la moyenne et de l'écart-type\n",
    "mu, sigma = norm.fit(x)  # Ajuste une gaussienne et retourne la moyenne et l'écart-type\n",
    "\n",
    "# Tracer l'histogramme des données\n",
    "plt.hist(x, bins=100, density=True, alpha=0.6, color='skyblue', label=\"Données\")\n",
    "\n",
    "# Créer une courbe gaussienne avec les paramètres ajustés\n",
    "x = np.linspace(min(x), max(x), 1000)\n",
    "pdf = norm.pdf(x, mu, sigma)  # Fonction de densité de probabilité\n",
    "plt.plot(x, pdf, color='red', linewidth=2, label=f\"Fit Gaussien\\n$\\mu$={mu:.2f}, $\\sigma$={sigma:.2f}\")\n",
    "\n",
    "# Ajouter des labels et une légende\n",
    "plt.xlabel(\"Valeurs\")\n",
    "plt.ylabel(\"Densité\")\n",
    "plt.title(\"Ajustement Gaussien\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Moyenne (mu): {mu:.2f}\")\n",
    "print(f\"Écart-type (sigma): {sigma:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_surface = df_cleaned['surface_reelle_bati'].quantile(0.1)\n",
    "upper_surface = df_cleaned['surface_reelle_bati'].quantile(0.95)\n",
    "mask_surface = df_cleaned['surface_reelle_bati'].between(lower_surface, upper_surface)\n",
    "\n",
    "lower_price_m2 = df_cleaned['valeur_fonciere_m2'].quantile(0.15)\n",
    "upper_price_m2 = df_cleaned['valeur_fonciere_m2'].quantile(0.85)\n",
    "mask_price_m2 = df_cleaned['valeur_fonciere_m2'].between(lower_price_m2, upper_price_m2)\n",
    "\n",
    "df_cleaned[mask_surface & mask_price_m2].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned[mask_price_m2 & mask_surface]\n",
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def idw_predict_kdtree(data, lon_col=\"longitude\", lat_col=\"latitude\", value_col=\"valeur_fonciere_m2\", power=2, k=20):\n",
    "    \"\"\"\n",
    "    Prédit les valeurs d'un DataFrame en utilisant l'Interpolation Inverse Distance Weighting (IDW) optimisée avec un KD-Tree.\n",
    "\n",
    "    Paramètres :\n",
    "        data : pd.DataFrame, DataFrame contenant les colonnes latitude, longitude et valeur foncière.\n",
    "        lat_col : str, nom de la colonne latitude.\n",
    "        lon_col : str, nom de la colonne longitude.\n",
    "        value_col : str, nom de la colonne des valeurs à prédire.\n",
    "        power : float, puissance de pondération (typiquement 2).\n",
    "        k : int, nombre de voisins à considérer pour chaque point.\n",
    "\n",
    "    Retourne :\n",
    "        pd.Series, colonne des valeurs prédites pour chaque point.\n",
    "    \"\"\"\n",
    "    # Extraire les coordonnées et les valeurs\n",
    "    coordinates = data[[lat_col, lon_col]].to_numpy()\n",
    "    values = data[value_col].to_numpy()\n",
    "\n",
    "    # Construire le KD-Tree\n",
    "    tree = cKDTree(coordinates,leafsize=36)\n",
    "\n",
    "    predicted_values = []\n",
    "    for i, point in enumerate(coordinates):\n",
    "        # Trouver les k voisins les plus proches\n",
    "        distances, indices = tree.query(point, k=k + 1)\n",
    "\n",
    "        # Exclure le point lui-même (distance 0)\n",
    "        mask = indices != i\n",
    "        indices = indices[mask][:k]  # Prendre les k voisins valides\n",
    "\n",
    "        # Si aucun voisin valide n'est trouvé\n",
    "        if len(indices) == 0:\n",
    "            predicted_values.append(values[i])  # Retourner la valeur du point lui-même\n",
    "            continue\n",
    "\n",
    "        # Calculer la valeur interpolée\n",
    "        mean_value = np.mean(values[indices])\n",
    "        predicted_values.append(mean_value)\n",
    "\n",
    "    # Retourner les résultats sous forme de pd.Series\n",
    "    return pd.Series(predicted_values, name=f\"{value_col}_predite_par_le_quartier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction idw_predict_kdtree et ajouter la colonne prédite au DataFrame pandas\n",
    "value_col = \"valeur_fonciere_m2\"\n",
    "df_cleaned[f\"{value_col}_predite_par_le_quartier\"] = idw_predict_kdtree(df_cleaned, lon_col=\"longitude\", lat_col=\"latitude\", value_col= value_col, k=36)\n",
    "df_cleaned = df_cleaned[~df_cleaned['valeur_fonciere_m2_predite_par_le_quartier'].isna()]\n",
    "df_cleaned = df_cleaned[np.isfinite(df_cleaned['valeur_fonciere_m2_predite_par_le_quartier'])]\n",
    "\n",
    "# Afficher les premières lignes du DataFrame\n",
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df_cleaned[df_cleaned['valeur_fonciere_m2']< 15000], x = \"valeur_fonciere_m2\", y = \"valeur_fonciere_m2_predite_par_le_quartier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(data=df_cleaned,x=\"valeur_fonciere_m2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "x = df_cleaned['valeur_fonciere_m2'].to_numpy()\n",
    "# Calcul de la moyenne et de l'écart-type\n",
    "mu, sigma = norm.fit(x)  # Ajuste une gaussienne et retourne la moyenne et l'écart-type\n",
    "\n",
    "# Tracer l'histogramme des données\n",
    "plt.hist(x, bins=100, density=True, alpha=0.6, color='skyblue', label=\"Données\")\n",
    "\n",
    "# Créer une courbe gaussienne avec les paramètres ajustés\n",
    "x = np.linspace(min(x), max(x), 1000)\n",
    "pdf = norm.pdf(x, mu, sigma)  # Fonction de densité de probabilité\n",
    "plt.plot(x, pdf, color='red', linewidth=2, label=f\"Fit Gaussien\\n$\\mu$={mu:.2f}, $\\sigma$={sigma:.2f}\")\n",
    "\n",
    "# Ajouter des labels et une légende\n",
    "plt.xlabel(\"Valeurs\")\n",
    "plt.ylabel(\"Densité\")\n",
    "plt.title(\"Ajustement Gaussien\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Moyenne (mu): {mu:.2f}\")\n",
    "print(f\"Écart-type (sigma): {sigma:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees = df_cleaned[(df_cleaned['valeur_fonciere_m2']<mu+3*sigma) & (df_cleaned['valeur_fonciere_m2']> mu-3*sigma)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_popd = pd.read_csv('data_pop_density/dataframe_densite&amenities_radius=500.csv')\n",
    "df_popd.drop(columns='Unnamed: 0',inplace=True)\n",
    "df_popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gamma\n",
    "\n",
    "# Exemple : données positives\n",
    "data_gamma = df_cleaned['valeur_fonciere_m2']\n",
    "\n",
    "# Ajuster les paramètres de la distribution gamma\n",
    "shape, loc, scale = gamma.fit(data_gamma, floc=0)  # floc=0 force le paramètre \"loc\" à zéro\n",
    "\n",
    "print(f\"Shape: {shape:.2f}, Scale: {scale:.2f}\")\n",
    "\n",
    "# Générer des valeurs pour tracer la densité de probabilité\n",
    "x = np.linspace(min(data_gamma), max(data_gamma), 500)\n",
    "pdf = gamma.pdf(x, shape, loc=loc, scale=scale)\n",
    "\n",
    "# Tracer les données et la distribution ajustée\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histogramme des données\n",
    "plt.hist(data_gamma, bins=50, density=True, alpha=0.6, color=\"skyblue\", label=\"Données\")\n",
    "\n",
    "# Courbe de la densité ajustée\n",
    "plt.plot(x, pdf, \"red\", label=\"Distribution Gamma ajustée\")\n",
    "\n",
    "# Ajouter des légendes et des titres\n",
    "plt.title(\"Ajustement de la distribution Gamma\", fontsize=14)\n",
    "plt.xlabel(\"valeur_fonciere_m2\", fontsize=12)\n",
    "plt.ylabel(\"Densité\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def find_weighted_poi_counts(donnees, df_grid, poi_columns):\n",
    "    \"\"\"\n",
    "    Calcule une moyenne pondérée des colonnes POIs des quatre voisins les plus proches\n",
    "    pour chaque point du DataFrame `donnees` en utilisant un arbre k-d (cKDTree).\n",
    "\n",
    "    Parameters:\n",
    "    - donnees (pandas.DataFrame): DataFrame contenant les coordonnées des points pour lesquels\n",
    "      les POIs pondérés doivent être calculés. Il doit contenir les colonnes 'latitude' et 'longitude'.\n",
    "    - df_grid (pandas.DataFrame): DataFrame contenant le quadrillage avec coordonnées et colonnes POIs.\n",
    "      Il doit contenir les colonnes 'lat', 'lon' et les colonnes spécifiées dans `poi_columns`.\n",
    "    - poi_columns (list): Liste des noms des colonnes POIs dans `df_grid` à inclure dans les calculs.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: DataFrame enrichi avec les colonnes pondérées des POIs.\n",
    "    \"\"\"\n",
    "    # Extraire les coordonnées des deux ensembles\n",
    "    latitudes_data = donnees['latitude'].values\n",
    "    longitudes_data = donnees['longitude'].values\n",
    "\n",
    "    latitudes_grid = df_grid['lat'].values\n",
    "    longitudes_grid = df_grid['lon'].values\n",
    "\n",
    "    # Créer un cKDTree pour une recherche rapide\n",
    "    tree = cKDTree(np.vstack((longitudes_grid, latitudes_grid)).T)\n",
    "\n",
    "    # Chercher les 4 voisins les plus proches pour chaque point\n",
    "    distances, indices = tree.query(np.vstack((longitudes_data, latitudes_data)).T, k=4)\n",
    "\n",
    "    # Calculer les poids en fonction de l'inverse des distances\n",
    "    weights = 1 / np.where(distances == 0, 1e-10, distances)  # Évite la division par zéro\n",
    "    normalized_weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Calculer les moyennes pondérées pour chaque colonne POI\n",
    "    for col in poi_columns:\n",
    "        poi_values = df_grid[col].values\n",
    "        # Récupérer les valeurs des voisins pour cette colonne\n",
    "        neighbors_poi = poi_values[indices]\n",
    "        # Calculer la moyenne pondérée\n",
    "        donnees[f\"{col}_weighted\"] = np.floor((neighbors_poi * normalized_weights).sum(axis=1))\n",
    "\n",
    "    return donnees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_columns = ['densite','transport_pois', 'education_pois','health_pois', 'food_pois', 'shopping_pois', 'park_pois',\t'entertainment_pois', 'cultural_pois']\n",
    "donnees = find_weighted_poi_counts(donnees=donnees,df_grid=df_popd,poi_columns = poi_columns)\n",
    "donnees.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = donnees.drop(['anomalie','valeur_fonciere_m2','valeur_fonciere_m2_log','valeur_fonciere_m2_predite_par_le_quartier'],axis = 1)\n",
    "# data_final[\"valeur_fonciere_log\"] = donnees['valeur_fonciere'].apply(lambda x: np.log10(x))\n",
    "# Remplacer les valeurs <NA> par NaN\n",
    "data_final['nature_culture'] = data_final['nature_culture'].replace(pd.NA, np.nan)\n",
    "data_final['nature_culture'] = data_final['nature_culture'].astype(str)\n",
    "data_final['type_local'] = data_final['type_local'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final['date_mutation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_final.drop(['valeur_fonciere'],axis =1)\n",
    "y= data_final['valeur_fonciere']\n",
    "categorical_columns_onehot = ['nature_culture','type_local'] # Columns that need OneHotEncoding\n",
    "numerical_columns = X.select_dtypes(include=['int32','float64']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and imputer Pipeline\n",
    "unique_categories = [X[col].unique() for col in categorical_columns_onehot]\n",
    "\n",
    "onehot_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', categories=unique_categories))\n",
    "])\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan,strategy='most_frequent')),  # Remplit les NaN avec 0\n",
    "    ('scaler', MinMaxScaler())                 # Standardisation\n",
    "])\n",
    "\n",
    "# Encoding pipeline\n",
    "\n",
    "column_transformer =  ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', onehot_pipeline, categorical_columns_onehot),\n",
    "        ('numeric', numeric_pipeline, numerical_columns)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_pipeline.fit(X[categorical_columns_onehot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipeline.fit(X[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "# Configuration pour afficher graphiquement\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "# Afficher la pipeline dans un notebook\n",
    "display(column_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xgboost_model():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', column_transformer),\n",
    "        ('model', XGBRegressor(\n",
    "            objective='reg:squarederror',  # Pour la régression\n",
    "            n_estimators=300,             # Nombre d'arbres\n",
    "            learning_rate=0.05,            # Taux d'apprentissage\n",
    "            max_depth=10,                  # Profondeur maximale des arbres\n",
    "            random_state=42               # Répétabilité\n",
    "        ))\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_xgboost_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test,y=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicabilité "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "# Supposons que ton modèle s'appelle `xgb_model`\n",
    "xgb_model = model.named_steps['model']\n",
    "# Récupérer l'importance des variables\n",
    "importance = xgb_model.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "# Convertir en DataFrame\n",
    "importance_df = pd.DataFrame.from_dict(importance, orient='index', columns=['weight']).reset_index()\n",
    "importance_df.columns = ['feature', 'weight']\n",
    "\n",
    "# Associer aux noms des colonnes transformées\n",
    "importance_df.sort_values(by='weight', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_importance(xgb_model, importance_type=\"weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Crée un explainer SHAP\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "\n",
    "# Calcule les valeurs SHAP pour les données d'entraînement/test\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Résumé global des contributions des variables\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# Visualiser l'impact d'une seule variable\n",
    "shap.dependence_plot(\"nom_de_la_variable\", shap_values, X_test)\n",
    "\n",
    "# Interprétation individuelle pour une observation spécifique\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Crée un explainer LIME\n",
    "explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=[\"Classe\"], mode=\"regression\")\n",
    "\n",
    "# Explique une prédiction individuelle\n",
    "exp = explainer.explain_instance(X_test.iloc[0].values, xgb_model.predict, num_features=10)\n",
    "\n",
    "# Affiche les explications\n",
    "exp.show_in_notebook(show_table=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# def build_deep_learning_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Dense(512, activation='relu', input_shape=(input_shape,)),\n",
    "#         Dense(256, activation='relu', input_shape=(input_shape,)),\n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dense(64, activation='relu'),\n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(256, activation='relu', input_shape=(input_shape,)),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(512, activation='relu', input_shape=(input_shape,)),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(1)  # Une seule sortie pour la régression\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['r2_score'])\n",
    "#     return model\n",
    "\n",
    "# def build_dl_pipeline(column_transformer, X_sample):\n",
    "#     # Ajuster le ColumnTransformer pour obtenir les dimensions de sortie\n",
    "#     column_transformer.fit(X_sample)\n",
    "#     transformed_sample = column_transformer.transform(X_sample)\n",
    "\n",
    "#     # Calculer la taille des features après transformation\n",
    "#     input_shape = transformed_sample.shape[1]\n",
    "#     print(f\"Input shape for Keras model: {input_shape}\")\n",
    "\n",
    "#     early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "#     reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "#                                   factor=0.2,    # Réduit le taux d'apprentissage par un facteur de 0.2\n",
    "#                                   patience=5,    # Attendre 5 epochs sans amélioration avant de réduire le taux\n",
    "#                                   min_lr=1e-6,   # Le taux d'apprentissage ne descendra pas en dessous de 1e-6\n",
    "#                                   verbose=1)\n",
    "\n",
    "#     keras_model = KerasRegressor(\n",
    "#         build_fn=build_deep_learning_model,\n",
    "#         input_shape=input_shape,  # Taille des features après prétraitement\n",
    "#         epochs=200,\n",
    "#         batch_size=16,\n",
    "#         verbose=1,\n",
    "#         callbacks = [early_stop,reduce_lr],\n",
    "#         validation_split = 0.2\n",
    "#     )\n",
    "\n",
    "#     pipeline = Pipeline(steps=[\n",
    "#         ('preprocessor', column_transformer),\n",
    "#         ('model', keras_model)\n",
    "#     ])\n",
    "#     return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_deep = build_dl_pipeline(column_transformer, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_transformer.fit_transform(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_deep.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred =model_deep.predict(X_test)\n",
    "# print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "# print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.scatterplot(x=y_test, y=y_pred, alpha=0.7, label='Prédictions')\n",
    "# sns.lineplot(x=y_test, y=y_test, color='red', label='Ligne parfaite (y_test = y_pred)')\n",
    "# plt.xlabel(\"Valeurs réelles (y_test)\")\n",
    "# plt.ylabel(\"Valeurs prédites (y_pred)\")\n",
    "# plt.title(\"Comparaison des valeurs réelles et prédites\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #best model\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# def build_deep_learning_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Dense(512, activation='relu', input_shape=(input_shape,)),\n",
    "#         Dense(256, activation='relu', input_shape=(input_shape,)),\n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dense(64, activation='relu'),\n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(256, activation='relu', input_shape=(input_shape,)),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(512, activation='relu', input_shape=(input_shape,)),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(1)  # Une seule sortie pour la régression\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['r2_score'])\n",
    "#     return model\n",
    "\n",
    "# def build_dl_pipeline(column_transformer, X_sample):\n",
    "#     # Ajuster le ColumnTransformer pour obtenir les dimensions de sortie\n",
    "#     column_transformer.fit(X_sample)\n",
    "#     transformed_sample = column_transformer.transform(X_sample)\n",
    "\n",
    "#     # Calculer la taille des features après transformation\n",
    "#     input_shape = transformed_sample.shape[1]\n",
    "#     print(f\"Input shape for Keras model: {input_shape}\")\n",
    "\n",
    "#     early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "#     reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "#                                   factor=0.2,    # Réduit le taux d'apprentissage par un facteur de 0.2\n",
    "#                                   patience=5,    # Attendre 5 epochs sans amélioration avant de réduire le taux\n",
    "#                                   min_lr=1e-6,   # Le taux d'apprentissage ne descendra pas en dessous de 1e-6\n",
    "#                                   verbose=1)\n",
    "\n",
    "#     keras_model = KerasRegressor(\n",
    "#         build_fn=build_deep_learning_model,\n",
    "#         input_shape=input_shape,  # Taille des features après prétraitement\n",
    "#         epochs=200,\n",
    "#         batch_size=16,\n",
    "#         verbose=1,\n",
    "#         callbacks = [early_stop,reduce_lr],\n",
    "#         validation_split = 0.2\n",
    "#     )\n",
    "\n",
    "#     pipeline = Pipeline(steps=[\n",
    "#         ('preprocessor', column_transformer),\n",
    "#         ('model', keras_model)\n",
    "#     ])\n",
    "#     return pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upfund_dvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
