{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)  # Toutes les colonnes\n",
    "pd.set_option('display.width', None)        # Pas de limite de largeur pour les lignes\n",
    "pd.set_option('display.max_colwidth', None) # Pas de limite pour la largeur des cellules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pl.read_csv('data_pop_density/dataframe_densite_amenities.csv')\n",
    "df_test.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupérations des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape de scrapping (Facultative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python script_scrapping_dvf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement dans un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_column_types(df):\n",
    "    \"\"\"\n",
    "    Change les types de plusieurs colonnes dans un DataFrame Polars.\n",
    "\n",
    "    Parameters:\n",
    "        df (pl.DataFrame): Le DataFrame Polars.\n",
    "        columns_dtypes (dict): Un dictionnaire où les clés sont les noms de colonnes\n",
    "                               et les valeurs sont les nouveaux types (ex: pl.Int64, pl.Float64).\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Un nouveau DataFrame avec les colonnes modifiées.\n",
    "    \"\"\"\n",
    "    dict_type ={'id_mutation': pl.Utf8, 'date_mutation': pl.Date, 'numero_disposition': pl.Utf8, 'nature_mutation': pl.Utf8, 'valeur_fonciere': pl.Float64, 'adresse_numero' : pl.Utf8, 'adresse_suffixe': pl.Utf8, 'adresse_nom_voie': pl.Utf8, 'adresse_code_voie' : pl.Utf8, 'code_postal':pl.Utf8, 'code_commune':pl.Utf8, 'nom_commune':pl.Utf8, 'code_departement':pl.Utf8, 'ancien_code_commune':pl.Utf8, 'ancien_nom_commune':pl.Utf8, 'id_parcelle':pl.Utf8, 'ancien_id_parcelle':pl.Utf8, 'numero_volume':pl.Utf8, 'lot1_numero':pl.Utf8, 'lot1_surface_carrez':pl.Float64, 'lot2_numero':pl.Utf8, 'lot2_surface_carrez':pl.Float64, 'lot3_numero':pl.Utf8, 'lot3_surface_carrez':pl.Float64, 'lot4_numero':pl.Utf8, 'lot4_surface_carrez':pl.Float64, 'lot5_numero':pl.Utf8, 'lot5_surface_carrez':pl.Float64, 'nombre_lots':pl.Float64, 'code_type_local': pl.Utf8, 'type_local': pl.Utf8, 'surface_reelle_bati': pl.Float64, 'nombre_pieces_principales': pl.Float64, 'code_nature_culture':pl.Utf8, 'nature_culture':pl.Utf8, 'code_nature_culture_speciale':pl.Utf8, 'nature_culture_speciale':pl.Utf8, 'surface_terrain':pl.Float64, 'longitude':pl.Float64, 'latitude':pl.Float64}\n",
    "    for column_name, column_type in dict_type.items():\n",
    "        df = df.with_columns(pl.col(column_name).cast(column_type))\n",
    "    return df\n",
    "\n",
    "def data_loader(path, departements = [], annees = []):\n",
    "\n",
    "    df = pl.DataFrame()\n",
    "    if not annees:\n",
    "        annees_list = os.listdir(path)\n",
    "    else:\n",
    "        annees_list =[str(annee) for annee in annees]\n",
    "    for annee in annees_list:\n",
    "        cur_year = os.path.join(path,annee)\n",
    "        if not departements:\n",
    "            departements_list = os.listdir(cur_year)\n",
    "        else:\n",
    "            departements_list = [f\"{departement}.csv.gz\" if departement>9 else f\"0{departement}.csv.gz\" for departement in departements]\n",
    "        for departement in departements_list:\n",
    "            file = os.path.join(cur_year,departement)\n",
    "            temp_df = pl.read_csv(file,ignore_errors=True)\n",
    "            temp_df = change_column_types(temp_df)\n",
    "            df = pl.concat([df, temp_df])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data_dvf'\n",
    "df = data_loader(path,departements=[75,92,93,94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\n",
    "    pl.col(\"valeur_fonciere\").is_not_null() &\n",
    "    pl.col(\"longitude\").is_not_null() &\n",
    "    pl.col(\"latitude\").is_not_null() &\n",
    "    (pl.col(\"surface_reelle_bati\").is_not_nan() | pl.col(\"surface_terrain\").is_not_nan())\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE : Prétraitement des données et premier modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['date_mutation','type_local','surface_reelle_bati','nombre_pieces_principales','nature_culture','surface_terrain','longitude','latitude','valeur_fonciere']]\n",
    "\n",
    "# # Calculer la composante saisonnière\n",
    "data = data.with_columns([\n",
    "    pl.col(\"date_mutation\").dt.month().map_batches(lambda x: np.sin(2 * np.pi * x / 12), return_dtype=pl.Float64).alias(\"sin_month\"),\n",
    "    pl.col(\"date_mutation\").dt.month().map_batches(lambda x: np.cos(2 * np.pi * x / 12), return_dtype=pl.Float64).alias(\"cos_month\"),\n",
    "    pl.col(\"date_mutation\").dt.year().alias('year')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_test = data.filter(pl.col('valeur_fonciere')<3e6)\n",
    "y = data_test['valeur_fonciere']\n",
    "X = data_test.drop(['valeur_fonciere'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_onehot = ['nature_culture','type_local'] # Columns that need OneHotEncoding\n",
    "numerical_columns = X.select(pl.col(pl.Float64)).columns # Numerical columns\n",
    "# Encoding and imputer Pipeline\n",
    "\n",
    "def replace_none_with_nan(X):\n",
    "    numerical_columns = X.select(pl.col(pl.Float64)).columns\n",
    "    for col in numerical_columns:\n",
    "        X = X.with_columns(\n",
    "            pl.col(col).cast(pl.Float64).fill_null(np.nan).alias(col)\n",
    "            )\n",
    "    return X\n",
    "\n",
    "onehot_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute les valeurs manquantes\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-Hot Encoding\n",
    "])\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('replace_null_to_nan', FunctionTransformer(replace_none_with_nan)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan,strategy='constant',fill_value=0)),  # Remplit les NaN avec 0\n",
    "    ('scaler', StandardScaler())                 # Standardisation\n",
    "])\n",
    "\n",
    "# Encoding pipeline\n",
    "\n",
    "column_transformer =  ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', onehot_pipeline, categorical_columns_onehot),\n",
    "        ('numeric', numeric_pipeline, numerical_columns)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "# Configuration pour afficher graphiquement\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "# Afficher la pipeline dans un notebook\n",
    "display(column_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "LR_pipeline = Pipeline([\n",
    "    ('preprocess', column_transformer),\n",
    "    ('linear_model',model)\n",
    "])\n",
    "display(LR_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pipeline.fit(X_train,y_train)\n",
    "y_pred = LR_pipeline.predict(X_test)\n",
    "\n",
    "r2 =  r2_score(y_test,y_pred)\n",
    "print(f' R² = {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(random_state=42, n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "lgb_pipeline = Pipeline([\n",
    "    ('preprocess', column_transformer),\n",
    "    ('linear_model',model)\n",
    "])\n",
    "display(lgb_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_pipeline.fit(X_train,y_train)\n",
    "y_pred = lgb_pipeline.predict(X_test)\n",
    "\n",
    "r2 =  r2_score(y_test,y_pred)\n",
    "print(f' R² = {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pl.DataFrame()\n",
    "validation = pl.DataFrame({\n",
    "    'y_test': y_test,\n",
    "    'y_pred': y_pred\n",
    "})\n",
    "\n",
    "# Tracer avec seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x=y_pred, y=y_test, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_test\")\n",
    "plt.xlim(0,5e6)\n",
    "plt.ylim(0,5e6)\n",
    "plt.title(\"Comparaison entre y_test et y_pred\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YToNumpyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        if isinstance(y, (pl.DataFrame, pl.Series)):\n",
    "            y = y.to_numpy()\n",
    "        return np.log(y)\n",
    "    def inverse_transform(self, y):\n",
    "        # Si aucune transformation inverse n'est nécessaire, renvoyez simplement y\n",
    "        return np.exp(y)\n",
    "\n",
    "class ToNumpyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Si X est un DataFrame ou une série Polars, le convertir en NumPy array\n",
    "        if isinstance(X, pl.DataFrame):\n",
    "            return X.to_numpy()\n",
    "        elif isinstance(X, pl.Series):\n",
    "            return X.to_numpy().reshape(-1, 1)\n",
    "        return X\n",
    "\n",
    "model = CatBoostRegressor(iterations=1000, depth=6, learning_rate=0.1, loss_function='RMSE')\n",
    "catboost_pipeline = Pipeline([\n",
    "    ('preprocess', column_transformer),\n",
    "    ('to_numpy',ToNumpyTransformer()),\n",
    "    ('catboost_model',model)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Pipeline pour X\n",
    "catboost_pipeline = Pipeline([\n",
    "    ('preprocess', column_transformer),\n",
    "    ('to_numpy', ToNumpyTransformer()),\n",
    "    ('catboost_model', model)\n",
    "])\n",
    "\n",
    "# Pipeline pour y avec TransformedTargetRegressor\n",
    "final_pipeline = TransformedTargetRegressor(\n",
    "    regressor=catboost_pipeline,\n",
    "    transformer=YToNumpyTransformer()\n",
    ")\n",
    "display(final_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline.fit(X_train,y_train)\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "r2 =  r2_score(y_test,y_pred)\n",
    "print(f' R² = {r2}')\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "print(f'MAPE: {mape:.2f}%')\n",
    "validation = pl.DataFrame()\n",
    "validation = pl.DataFrame({\n",
    "    'y_test': y_test,\n",
    "    'y_pred': y_pred\n",
    "})\n",
    "\n",
    "# Tracer avec seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x=y_pred, y=y_test, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"y_test\")\n",
    "plt.xlim(0,5e6)\n",
    "plt.ylim(0,5e6)\n",
    "plt.title(\"Comparaison entre y_test et y_pred\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.xlim(0,0.4e7)\n",
    "plt.ylim(-0.2e7,0.3e7)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Valeurs prédites')\n",
    "plt.ylabel('Résidus')\n",
    "plt.title('Graphique des résidus')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup d'hétéroscédasticité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele plus puissant: Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data_dvf'\n",
    "df = data_loader(path,departements=[75,92,93,94])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\n",
    "    pl.col(\"valeur_fonciere\").is_not_null() &\n",
    "    pl.col(\"longitude\").is_not_null() &\n",
    "    pl.col(\"latitude\").is_not_null() &\n",
    "    (pl.col(\"surface_reelle_bati\").is_not_nan() | pl.col(\"surface_terrain\").is_not_nan())\n",
    ")\n",
    "\n",
    "data = df[['date_mutation','type_local','surface_reelle_bati','nombre_lots','lot1_surface_carrez','lot2_surface_carrez','lot3_surface_carrez','lot4_surface_carrez','lot5_surface_carrez','nombre_pieces_principales','nature_culture','surface_terrain','longitude','latitude','valeur_fonciere']]\n",
    "\n",
    "\n",
    "data = data.with_columns([\n",
    "    pl.col(\"date_mutation\").dt.month().map_batches(lambda x: np.sin(2 * np.pi * x / 12), return_dtype=pl.Float64).alias(\"sin_month\"),\n",
    "    pl.col(\"date_mutation\").dt.month().map_batches(lambda x: np.cos(2 * np.pi * x / 12), return_dtype=pl.Float64).alias(\"cos_month\"),\n",
    "    pl.col(\"date_mutation\").dt.year().alias('year'),\n",
    "    pl.col('lot1_surface_carrez').fill_null(value=0),\n",
    "    pl.col('lot2_surface_carrez').fill_null(value=0),\n",
    "    pl.col('lot3_surface_carrez').fill_null(value=0),\n",
    "    pl.col('lot4_surface_carrez').fill_null(value=0),\n",
    "    pl.col('lot5_surface_carrez').fill_null(value=0),\n",
    "    pl.col('surface_reelle_bati').fill_null(value=0),\n",
    "    pl.col('surface_terrain').fill_null(value=0)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.with_columns(\n",
    "    (pl.col('lot1_surface_carrez')+pl.col('lot2_surface_carrez')+pl.col('lot3_surface_carrez')+pl.col('lot4_surface_carrez')+pl.col('lot5_surface_carrez')\n",
    ").alias(\"total_surface_carrez\"))\n",
    "\n",
    "data = data.with_columns(\n",
    "    pl.when(pl.col(\"total_surface_carrez\") == 0)\n",
    "            .then(pl.col(\"surface_reelle_bati\"))\n",
    "            .otherwise(pl.col(\"total_surface_carrez\"))\n",
    "            .alias(\"total_surface_carrez\"),\n",
    "    pl.when(pl.col(\"surface_reelle_bati\") != 0)\n",
    "            .then((pl.col(\"valeur_fonciere\")/pl.col(\"surface_reelle_bati\")).round(0))\n",
    "            .otherwise((pl.col(\"valeur_fonciere\")/pl.col(\"surface_terrain\")).round(0))\n",
    "            .alias(\"valeur_fonciere_m2\")\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "def idw_predict_kdtree(data, lon_col=\"longitude\", lat_col=\"latitude\", value_col=\"valeur_fonciere_m2\", power=2, k=10):\n",
    "    \"\"\"\n",
    "    Prédit les valeurs d'un DataFrame en utilisant l'Interpolation Inverse Distance Weighting (IDW) optimisée avec un KD-Tree.\n",
    "\n",
    "    Paramètres :\n",
    "        data : pl.DataFrame, DataFrame contenant les colonnes latitude, longitude et valeur foncière.\n",
    "        lat_col : str, nom de la colonne latitude.\n",
    "        lon_col : str, nom de la colonne longitude.\n",
    "        value_col : str, nom de la colonne des valeurs à prédire.\n",
    "        power : float, puissance de pondération (typiquement 2).\n",
    "        k : int, nombre de voisins à considérer pour chaque point.\n",
    "\n",
    "    Retourne :\n",
    "        pl.Series, colonne des valeurs prédites pour chaque point.\n",
    "    \"\"\"\n",
    "    # Extraire les coordonnées et les valeurs\n",
    "    coordinates = data.select([lat_col, lon_col]).to_numpy()\n",
    "    values = data[value_col].to_numpy()\n",
    "\n",
    "    # Construire le KD-Tree\n",
    "    tree = cKDTree(coordinates)\n",
    "\n",
    "    predicted_values = []\n",
    "    for i, point in enumerate(coordinates):\n",
    "        # Trouver les k voisins les plus proches\n",
    "        distances, indices = tree.query(point, k=k + 1)  # k+1 car le point lui-même est inclus\n",
    "\n",
    "        # Exclure le point lui-même (distance 0)\n",
    "        mask = distances > 0\n",
    "        distances = distances[mask]\n",
    "        indices = indices[mask]\n",
    "\n",
    "        # Si aucun voisin valide n'est trouvé\n",
    "        if len(distances) == 0:\n",
    "            predicted_values.append(values[i])  # Retourner la valeur du point lui-même\n",
    "            continue\n",
    "\n",
    "        # Calculer les poids en fonction des distances\n",
    "        weights = 1 / (distances ** power)\n",
    "\n",
    "        # Calculer la valeur interpolée\n",
    "        interpolated_value = np.round(np.sum(weights * values[indices]) / np.sum(weights),-1)\n",
    "        predicted_values.append(interpolated_value)\n",
    "\n",
    "    return pl.Series(predicted_values).alias(f\"{value_col}_predite_par_le_quartier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.with_columns(\n",
    "    idw_predict_kdtree(data, lon_col=\"longitude\",lat_col=\"latitude\", value_col=\"valeur_fonciere_m2\", k=100),\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_popd = pd.read_csv('data_pop_density/dataframe_densite&amenities_radius=500.csv')\n",
    "df_popd.drop(columns='Unnamed: 0',inplace=True)\n",
    "df_popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy.spatial import cKDTree  # Importer cKDTree pour une recherche efficace des plus proches voisins\n",
    "\n",
    "# Fonction pour trouver la densité la plus proche en utilisant cKDTree\n",
    "def find_nearest_square_data(data, df_popd):\n",
    "    \"\"\"\n",
    "    Trouve la densité de population du point le plus proche pour chaque point dans un DataFrame de données\n",
    "    géographiques en utilisant un arbre k-d (cKDTree).\n",
    "\n",
    "    Cette fonction cherche, pour chaque point dans `data`, le point le plus proche dans `df_popd`\n",
    "    (en fonction de la distance géographique) et retourne la densité associée à ce point.\n",
    "\n",
    "    Parameters:\n",
    "    - data (polars.DataFrame): Le DataFrame contenant les coordonnées géographiques des points\n",
    "      pour lesquels la densité doit être calculée. Il doit contenir les colonnes 'latitude' et 'longitude'.\n",
    "    - df_popd (polars.DataFrame): Le DataFrame contenant les coordonnées géographiques des points\n",
    "      de population et la densité associée. Il doit contenir les colonnes 'lat', 'lon' et 'densite'.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Un tableau contenant les densités associées aux plus proches voisins pour\n",
    "      chaque point de `data`.\n",
    "    \"\"\"\n",
    "    latitudes_data = data['latitude'].to_numpy()\n",
    "    longitudes_data = data['longitude'].to_numpy()\n",
    "\n",
    "    latitudes_popd = df_popd['lat'].to_numpy()\n",
    "    longitudes_popd = df_popd['lon'].to_numpy()\n",
    "    densities_popd = df_popd['densite'].to_numpy()\n",
    "\n",
    "    # Créer un cKDTree pour une recherche rapide des plus proches voisins\n",
    "    tree = cKDTree(np.vstack((longitudes_popd, latitudes_popd)).T)\n",
    "\n",
    "    # Chercher les voisins les plus proches pour chaque point de `data`\n",
    "    distances, indices = tree.query(np.vstack((longitudes_data, latitudes_data)).T, k=4)\n",
    "\n",
    "    # Récupérer les densités associées aux plus proches voisins\n",
    "    nearest_densities = np.mean(densities_popd[indices], axis=1)\n",
    "\n",
    "    return nearest_densities\n",
    "\n",
    "# Appliquer la fonction pour ajouter la colonne 'nearest_density' au DataFrame\n",
    "nearest_densities = find_nearest_square_data(data, df_popd)\n",
    "\n",
    "# Ajouter la colonne 'nearest_density' à ton DataFrame Polars\n",
    "data = data.with_columns(\n",
    "    pl.Series('nearest_density', nearest_densities)\n",
    ")\n",
    "\n",
    "# Afficher le DataFrame résultant\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "def find_weighted_poi_counts(data, df_grid, poi_columns):\n",
    "    \"\"\"\n",
    "    Calcule une moyenne pondérée des colonnes POIs des quatre voisins les plus proches\n",
    "    pour chaque point du DataFrame `data` en utilisant un arbre k-d (cKDTree).\n",
    "\n",
    "    Parameters:\n",
    "    - data (polars.DataFrame): DataFrame contenant les coordonnées des points pour lesquels\n",
    "      les POIs pondérés doivent être calculés. Il doit contenir les colonnes 'latitude' et 'longitude'.\n",
    "    - df_grid (polars.DataFrame): DataFrame contenant le quadrillage avec coordonnées et colonnes POIs.\n",
    "      Il doit contenir les colonnes 'lat', 'lon' et les colonnes spécifiées dans `poi_columns`.\n",
    "    - poi_columns (list): Liste des noms des colonnes POIs dans `df_grid` à inclure dans les calculs.\n",
    "\n",
    "    Returns:\n",
    "    - polars.DataFrame: DataFrame enrichi avec les colonnes pondérées des POIs.\n",
    "    \"\"\"\n",
    "    # Extraire les coordonnées des deux ensembles\n",
    "    latitudes_data = data['latitude'].to_numpy()\n",
    "    longitudes_data = data['longitude'].to_numpy()\n",
    "\n",
    "    latitudes_grid = df_grid['lat'].to_numpy()\n",
    "    longitudes_grid = df_grid['lon'].to_numpy()\n",
    "\n",
    "    # Créer un cKDTree pour une recherche rapide\n",
    "    tree = cKDTree(np.vstack((longitudes_grid, latitudes_grid)).T)\n",
    "\n",
    "    # Chercher les 4 voisins les plus proches pour chaque point\n",
    "    distances, indices = tree.query(np.vstack((longitudes_data, latitudes_data)).T, k=4)\n",
    "\n",
    "    # Calculer les poids en fonction de l'inverse des distances\n",
    "    weights = 1 / np.where(distances == 0, 1e-10, distances)  # Évite la division par zéro\n",
    "    normalized_weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Calculer les moyennes pondérées pour chaque colonne POI\n",
    "    weighted_poi_data = {}\n",
    "    for col in poi_columns:\n",
    "        poi_values = df_grid[col].to_numpy()\n",
    "        # Récupérer les valeurs des voisins pour cette colonne\n",
    "        neighbors_poi = poi_values[indices]\n",
    "        # Calculer la moyenne pondérée\n",
    "        weighted_poi_data[f\"{col}_weighted\"] = np.floor((neighbors_poi * normalized_weights).sum(axis=1))\n",
    "\n",
    "    # Ajouter les colonnes pondérées au DataFrame d'entrée\n",
    "    return data.with_columns([\n",
    "        pl.Series(name, values) for name, values in weighted_poi_data.items()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes POIs à inclure dans les calculs\n",
    "poi_columns = ['transport_pois', 'education_pois','health_pois', 'food_pois', 'shopping_pois', 'park_pois',\t'entertainment_pois', 'cultural_pois']\n",
    "\n",
    "# Calculer les moyennes pondérées\n",
    "data_expanded = find_weighted_poi_counts(data,df_popd, poi_columns)\n",
    "\n",
    "data_expanded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Conversion en pandas\n",
    "df_pandas = data_expanded.to_pandas()\n",
    "\n",
    "# Tracer avec Seaborn\n",
    "sns.scatterplot(data=df_pandas, x=\"valeur_fonciere_m2\", y=\"valeur_fonciere_m2_predite_par_le_quartier\")\n",
    "plt.plot([df_pandas[\"valeur_fonciere_m2\"].min(), df_pandas[\"valeur_fonciere_m2\"].max()],\n",
    "         [df_pandas[\"valeur_fonciere_m2\"].min(), df_pandas[\"valeur_fonciere_m2\"].max()],\n",
    "         color='red', linestyle='--', label=\"y=x Line\")\n",
    "plt.title(\"Scatter plot of valeur_fonciere_m2 vs valeur_fonciere_m2_predite_par_le_quartier\")\n",
    "plt.xlim((0,0.5e8))\n",
    "plt.ylim((0,0.5e8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas['valeur_fonciere_m2_log']=df_pandas['valeur_fonciere_m2'].apply(lambda X: np.log10(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_pandas,x='valeur_fonciere_m2_log',bins=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_pandas['valeur_fonciere_m2_log']<5.5) & (df_pandas['total_surface_carrez']+df_pandas['surface_terrain']>1) & (df_pandas[\"nombre_pieces_principales\"].notnull())\n",
    "test_df =df_pandas[mask]\n",
    "test_df['valeur_fonciere_m2_predite_par_le_quartier_log']=test_df['valeur_fonciere_m2_predite_par_le_quartier'].apply(lambda X: np.log10(X))\n",
    "sns.histplot(data=test_df,x='valeur_fonciere_m2_predite_par_le_quartier_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.sort_values(by='surface_terrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Liste des colonnes à tracer\n",
    "columns_to_plot = [\n",
    "    'valeur_fonciere_m2_predite_par_le_quartier_log',\n",
    "    'type_local',\n",
    "    'total_surface_carrez'\n",
    "]\n",
    "\n",
    "# Nombre de colonnes pour l'affichage des subplots\n",
    "n_cols = len(columns_to_plot)\n",
    "\n",
    "# Configurer la figure avec plusieurs sous-graphiques\n",
    "fig, axes = plt.subplots(1, n_cols, figsize=(5 * n_cols, 5))\n",
    "\n",
    "# Tracer chaque histogramme\n",
    "for ax, column in zip(axes, columns_to_plot):\n",
    "    sns.histplot(data=test_df, x=column, ax=ax, color='skyblue')\n",
    "    ax.set_title(column)\n",
    "\n",
    "# Ajuster les espacements\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Exemple de DataFrame pandas\n",
    "df_pandas_sampled = df_pandas.sample(10000)  # Prendre un échantillon de 10 000 points\n",
    "\n",
    "# Création de la carte avec Plotly Express\n",
    "fig = px.scatter_mapbox(\n",
    "    df_pandas_sampled,\n",
    "    lat=\"latitude\",  # Colonne pour les latitudes\n",
    "    lon=\"longitude\",  # Colonne pour les longitudes\n",
    "    size=\"valeur_fonciere_m2\",  # Taille des cercles basée sur la valeur/m²\n",
    "    color=\"valeur_fonciere_m2\",  # Couleur des cercles basée sur la valeur/m²\n",
    "    color_continuous_scale=px.colors.sequential.Blues,  # Palette de couleurs\n",
    "    size_max=15,  # Taille maximale des cercles\n",
    "    zoom=13,  # Niveau de zoom initial\n",
    "    mapbox_style=\"carto-positron\",  # Style de carte\n",
    "    title=\"Carte des valeurs foncières au m²\"\n",
    ")\n",
    "\n",
    "# Ajouter des popups avec les informations\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<b>Prix/m²</b>: %{marker.size} €<br><b>Latitude</b>: %{lat}<br><b>Longitude</b>: %{lon}\"\n",
    ")\n",
    "\n",
    "# Afficher la carte\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upfund_dvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
