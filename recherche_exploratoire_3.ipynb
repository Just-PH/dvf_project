{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)  # Toutes les colonnes\n",
    "pd.set_option('display.width', None)        # Pas de limite de largeur pour les lignes\n",
    "pd.set_option('display.max_colwidth', None) # Pas de limite pour la largeur des cellules\n",
    "\n",
    "def change_column_types(df):\n",
    "    \"\"\"\n",
    "    Change les types de plusieurs colonnes dans un DataFrame pandas.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Le DataFrame pandas.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un nouveau DataFrame avec les colonnes modifiées.\n",
    "    \"\"\"\n",
    "    # Dictionnaire des colonnes et leurs types cibles\n",
    "    dict_type = {\n",
    "        'id_mutation': 'string',\n",
    "        'date_mutation': 'datetime64[ns]',\n",
    "        'numero_disposition': 'string',\n",
    "        'nature_mutation': 'string',\n",
    "        'valeur_fonciere': 'float64',\n",
    "        'adresse_numero': 'string',\n",
    "        'adresse_suffixe': 'string',\n",
    "        'adresse_nom_voie': 'string',\n",
    "        'adresse_code_voie': 'string',\n",
    "        'code_postal': 'string',\n",
    "        'code_commune': 'string',\n",
    "        'nom_commune': 'string',\n",
    "        'code_departement': 'string',\n",
    "        'ancien_code_commune': 'string',\n",
    "        'ancien_nom_commune': 'string',\n",
    "        'id_parcelle': 'string',\n",
    "        'ancien_id_parcelle': 'string',\n",
    "        'numero_volume': 'string',\n",
    "        'lot1_numero': 'string',\n",
    "        'lot1_surface_carrez': 'float64',\n",
    "        'lot2_numero': 'string',\n",
    "        'lot2_surface_carrez': 'float64',\n",
    "        'lot3_numero': 'string',\n",
    "        'lot3_surface_carrez': 'float64',\n",
    "        'lot4_numero': 'string',\n",
    "        'lot4_surface_carrez': 'float64',\n",
    "        'lot5_numero': 'string',\n",
    "        'lot5_surface_carrez': 'float64',\n",
    "        'nombre_lots': 'float64',\n",
    "        'code_type_local': 'string',\n",
    "        'type_local': 'string',\n",
    "        'surface_reelle_bati': 'float64',\n",
    "        'nombre_pieces_principales': 'float64',\n",
    "        'code_nature_culture': 'string',\n",
    "        'nature_culture': 'string',\n",
    "        'code_nature_culture_speciale': 'string',\n",
    "        'nature_culture_speciale': 'string',\n",
    "        'surface_terrain': 'float64',\n",
    "        'longitude': 'float64',\n",
    "        'latitude': 'float64'\n",
    "    }\n",
    "\n",
    "    # Conversion des types de colonnes\n",
    "    for column_name, column_type in dict_type.items():\n",
    "        if column_name in df.columns:  # Vérifie que la colonne existe\n",
    "            try:\n",
    "                df[column_name] = df[column_name].astype(column_type)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la conversion de la colonne {column_name}: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def data_loader(path, departements=[], annees=[], fraction = None, chunksize=None):\n",
    "    \"\"\"\n",
    "    Charge les fichiers CSV d'un répertoire et les concatène dans un DataFrame pandas.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Chemin vers le dossier contenant les données.\n",
    "        departements (list): Liste des départements à inclure (ex: [1, 75]).\n",
    "        annees (list): Liste des années à inclure (ex: [2020, 2021]).\n",
    "        fraction (float): Fraction des données à charger (ex: 0.1 pour 10%). Si None, charge toutes les données.\n",
    "        chunksize (int): Taille des chunks à charger (en nombre de lignes). Si None, charge tout le fichier d'un coup.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame pandas contenant toutes les données chargées.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Liste des années\n",
    "    annees_list = os.listdir(path) if not annees else [str(annee) for annee in annees]\n",
    "\n",
    "    for annee in annees_list:\n",
    "        cur_year = os.path.join(path, annee)\n",
    "\n",
    "        # Liste des départements\n",
    "        if not departements:\n",
    "            departements_list = os.listdir(cur_year)\n",
    "        else:\n",
    "            departements_list = [f\"{departement:02}.csv.gz\" for departement in departements]\n",
    "\n",
    "        for departement in departements_list:\n",
    "\n",
    "            file = os.path.join(cur_year, departement)\n",
    "\n",
    "            try:\n",
    "                # Charger les données par chunks si chunksize est spécifié\n",
    "                if chunksize:\n",
    "                    chunk_list = []\n",
    "                    for chunk in pd.read_csv(file, chunksize=chunksize, low_memory=False):\n",
    "                        # Appliquer la fraction sur chaque chunk si nécessaire\n",
    "                        if fraction is not None:\n",
    "                            chunk = chunk.sample(frac=fraction, random_state=42)\n",
    "                        chunk = change_column_types(chunk)\n",
    "                        chunk_list.append(chunk)\n",
    "\n",
    "                    temp_df = pd.concat(chunk_list, ignore_index=True)\n",
    "\n",
    "                else:\n",
    "                    # Charger le fichier complet si chunksize n'est pas spécifié\n",
    "                    temp_df = pd.read_csv(file, low_memory=False)\n",
    "                    if fraction is not None:\n",
    "                        temp_df = temp_df.sample(frac=fraction, random_state=42)\n",
    "                    temp_df = change_column_types(temp_df)\n",
    "\n",
    "                # Concaténer les données\n",
    "                df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du chargement du fichier {file}: {e}\")\n",
    "\n",
    "    return df.dropna(subset=['valeur_fonciere'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Nettoyage des données (Colonnes nécessaires et NaN)\n",
    "class DataCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X[[\n",
    "                'id_mutation','date_mutation', 'type_local', 'surface_reelle_bati',\n",
    "                'nombre_lots', 'lot1_surface_carrez', 'lot2_surface_carrez',\n",
    "                'lot3_surface_carrez', 'lot4_surface_carrez', 'lot5_surface_carrez',\n",
    "                'nombre_pieces_principales', 'surface_terrain', 'longitude', 'latitude',\n",
    "                'valeur_fonciere'\n",
    "            ]]\n",
    "        X = X[\n",
    "            (X[\"valeur_fonciere\"].notna())\n",
    "            & (X[\"longitude\"].notna())\n",
    "            & (X[\"latitude\"].notna())\n",
    "            & (X[\"surface_reelle_bati\"].notna() | X[\"surface_terrain\"].notna())\n",
    "            & (X[\"nombre_lots\"] <= 5)\n",
    "\n",
    "        ]\n",
    "        return X\n",
    "\n",
    "# Étape 2 : Création de colonnes calculées\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Ajout de transformations basées sur X\n",
    "        X = X.assign(\n",
    "            sin_month=np.sin(2 * np.pi * pd.to_datetime(X[\"date_mutation\"]).dt.month / 12),\n",
    "            cos_month=np.cos(2 * np.pi * pd.to_datetime(X[\"date_mutation\"]).dt.month / 12),\n",
    "            year=pd.to_datetime(X[\"date_mutation\"]).dt.year,\n",
    "            lot1_surface_carrez=X['lot1_surface_carrez'].fillna(0),\n",
    "            lot2_surface_carrez=X['lot2_surface_carrez'].fillna(0),\n",
    "            lot3_surface_carrez=X['lot3_surface_carrez'].fillna(0),\n",
    "            lot4_surface_carrez=X['lot4_surface_carrez'].fillna(0),\n",
    "            lot5_surface_carrez=X['lot5_surface_carrez'].fillna(0),\n",
    "            surface_reelle_bati=X['surface_reelle_bati'].fillna(0),\n",
    "            surface_terrain=X['surface_terrain'].fillna(0)\n",
    "        )\n",
    "        X = X.drop('date_mutation',axis = 1)\n",
    "        X[\"total_surface_carrez\"] = (\n",
    "            X[\"lot1_surface_carrez\"] +\n",
    "            X[\"lot2_surface_carrez\"] +\n",
    "            X[\"lot3_surface_carrez\"] +\n",
    "            X[\"lot4_surface_carrez\"] +\n",
    "            X[\"lot5_surface_carrez\"]\n",
    "        )\n",
    "        X[\"total_surface_carrez\"] = np.where(\n",
    "            X[\"total_surface_carrez\"] == 0,\n",
    "            X[\"surface_reelle_bati\"],\n",
    "            X[\"total_surface_carrez\"]\n",
    "        )\n",
    "        X=X.groupby(by=['id_mutation', 'valeur_fonciere'], as_index=False)\\\n",
    "            .agg({\n",
    "                'surface_reelle_bati': 'sum',\n",
    "                'year' : 'first',\n",
    "                'sin_month' : 'first',\n",
    "                'cos_month' : 'first',\n",
    "                'type_local' : lambda x: ', '.join(sorted(x.dropna().unique())) if x.notna().any() else 'None',\n",
    "                'nombre_lots' : 'max',\n",
    "                'lot1_surface_carrez': 'mean',\n",
    "                'lot2_surface_carrez': 'mean',\n",
    "                'lot3_surface_carrez': 'mean',\n",
    "                'lot4_surface_carrez': 'mean',\n",
    "                'lot5_surface_carrez': 'mean',\n",
    "                'nombre_pieces_principales': 'sum',\n",
    "                'surface_terrain': 'sum',\n",
    "                'longitude' : 'first',\n",
    "                'latitude' : 'first'\n",
    "            })\n",
    "        # X[\"valeur_fonciere_m2\"] = np.where(\n",
    "        #     X[\"surface_reelle_bati\"] != 0,\n",
    "        #     (X[\"valeur_fonciere\"] / X[\"surface_reelle_bati\"]).round(0),\n",
    "        #     (X[\"valeur_fonciere\"] / X[\"surface_terrain\"]).round(0)\n",
    "        # )\n",
    "        # X[\"valeur_fonciere_m2_log\"] = X[\"valeur_fonciere_m2\"].apply(lambda x: np.log10(x) if x > 0 else np.nan)\n",
    "\n",
    "        return X.drop('id_mutation',axis = 1)\n",
    "\n",
    "# Étape 3 : Filtrage des anomalies\n",
    "class AnomalyFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, contamination=0.1):\n",
    "        self.contamination = contamination\n",
    "        self.model = IsolationForest(contamination=self.contamination, random_state=42)\n",
    "        self.anomaly_columns =['surface_reelle_bati', 'nombre_lots', 'surface_terrain', 'nombre_pieces_principales']\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X[self.anomaly_columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[\"anomalie\"] = self.model.predict(X[self.anomaly_columns])\n",
    "        X = X[X[\"anomalie\"] == 1]\n",
    "        return X.drop(columns=[\"anomalie\"])\n",
    "\n",
    "# Étape 4 : ajout des données de densité et de pois\n",
    "class WeightedPOICountsTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Calcule une moyenne pondérée des colonnes POIs des voisins les plus proches pour chaque point\n",
    "    en utilisant un arbre k-d (cKDTree).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_neighbors=4, df_grid=None):\n",
    "        \"\"\"\n",
    "        Initialise le transformateur.\n",
    "\n",
    "        Parameters:\n",
    "        - poi_columns (list): Liste des noms des colonnes POIs à inclure dans les calculs.\n",
    "        - n_neighbors (int): Nombre de voisins à prendre en compte pour le calcul des POIs pondérés.\n",
    "        \"\"\"\n",
    "        self.poi_columns = ['densite', 'transport_pois', 'education_pois', 'health_pois', 'food_pois',\n",
    "                                'shopping_pois', 'park_pois', 'entertainment_pois', 'cultural_pois']\n",
    "\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.df_grid = df_grid\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Applique la transformation pour calculer les POIs pondérés sur les données.\n",
    "\n",
    "        Parameters:\n",
    "        - X (pandas.DataFrame): DataFrame contenant les coordonnées des points pour lesquels\n",
    "          les POIs pondérés doivent être calculés.\n",
    "\n",
    "        Returns:\n",
    "        - pandas.DataFrame: DataFrame enrichi avec les colonnes pondérées des POIs.\n",
    "        \"\"\"\n",
    "        if self.df_grid is None:\n",
    "            raise ValueError(\"df_grid doit être fourni dans fit avant d'appeler transform.\")\n",
    "\n",
    "        # Extraire les coordonnées des deux ensembles\n",
    "        latitudes_data = X['latitude'].values\n",
    "        longitudes_data = X['longitude'].values\n",
    "\n",
    "        latitudes_grid = self.df_grid['lat'].values\n",
    "        longitudes_grid = self.df_grid['lon'].values\n",
    "\n",
    "        assert np.all(np.isfinite(longitudes_grid)), \"longitudes_grid contient des valeurs non finies.\"\n",
    "        assert np.all(np.isfinite(latitudes_grid)), \"latitudes_grid contient des valeurs non finies.\"\n",
    "        assert np.all(np.isfinite(longitudes_data)), \"longitudes_data contient des valeurs non finies.\"\n",
    "        assert np.all(np.isfinite(latitudes_data)), \"latitudes_data contient des valeurs non finies.\"\n",
    "\n",
    "        # Créer un cKDTree pour une recherche rapide sur df_grid\n",
    "        tree = cKDTree(np.vstack((longitudes_grid, latitudes_grid)).T)\n",
    "\n",
    "        # Chercher les n_neighbors voisins les plus proches pour chaque point de X\n",
    "        distances, indices = tree.query(np.vstack((longitudes_data, latitudes_data)).T, k=self.n_neighbors)\n",
    "\n",
    "        # Calculer les poids en fonction de l'inverse des distances\n",
    "        weights = 1 / np.where(distances == 0, 1e-10, distances)  # Évite la division par zéro\n",
    "        normalized_weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Calculer les moyennes pondérées pour chaque colonne POI\n",
    "        for col in self.poi_columns:\n",
    "            poi_values = self.df_grid[col].values  # Utiliser les POIs de df_grid\n",
    "            # Récupérer les valeurs des voisins pour cette colonne\n",
    "            neighbors_poi = poi_values[indices]\n",
    "            # Calculer la moyenne pondérée\n",
    "            X[f\"{col}_weighted\"] = np.floor((neighbors_poi * normalized_weights).sum(axis=1))\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "# Pipeline complète\n",
    "pipeline_preprocess = Pipeline(steps=[\n",
    "    (\"cleaner\", DataCleaner()),\n",
    "    (\"feature_creator\", FeatureCreator()),\n",
    "    (\"anomaly_filter\", AnomalyFilter(contamination=0.1)),\n",
    "    ('weighted_poi', WeightedPOICountsTransformer(n_neighbors=4)),\n",
    "])\n",
    "\n",
    "# pipeline_preprocess_test = Pipeline(steps=[\n",
    "#     (\"cleaner\", DataCleaner()),\n",
    "#     (\"feature_creator\", FeatureCreator()),\n",
    "#     (\"anomaly_filter\", AnomalyFilter(contamination=0.1)),\n",
    "#     (\"quantile_filter\", QuantileFilter()),\n",
    "#     (\"statistical_filter\", StatisticalFilter(sigma_multiplier=3)),\n",
    "#     ('weighted_poi', WeightedPOICountsTransformer(n_neighbors=4)),\n",
    "#     ('feature_remover', FeatureRemover())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data_dvf'\n",
    "df = data_loader(path,annees=['2023'],departements=['75']) #\n",
    "\n",
    "df_grid = pd.read_csv('data_pop_density/dataframe_densite&amenities_radius=500.csv')\n",
    "df_grid.drop(columns='Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_preprocess.set_params(weighted_poi__df_grid = df_grid)\n",
    "\n",
    "df = pipeline_preprocess.fit_transform(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la pipeline\n",
    "X = df.drop('valeur_fonciere',axis = 1)\n",
    "y= df['valeur_fonciere']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and imputer Pipeline\n",
    "\n",
    "categorical_columns_onehot = ['type_local'] # Columns that need OneHotEncoding\n",
    "numerical_columns = ['surface_reelle_bati', 'year', 'sin_month',\n",
    "       'cos_month', 'nombre_lots', 'lot1_surface_carrez',\n",
    "       'lot2_surface_carrez', 'lot3_surface_carrez', 'lot4_surface_carrez',\n",
    "       'lot5_surface_carrez', 'nombre_pieces_principales', 'surface_terrain',\n",
    "       'longitude', 'latitude', 'densite_weighted', 'transport_pois_weighted',\n",
    "       'education_pois_weighted', 'health_pois_weighted', 'food_pois_weighted',\n",
    "       'shopping_pois_weighted', 'park_pois_weighted',\n",
    "       'entertainment_pois_weighted', 'cultural_pois_weighted']\n",
    "unique_categories = [X[col].dropna().unique() for col in categorical_columns_onehot]\n",
    "onehot_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', categories=unique_categories))\n",
    "])\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=pd.NA,strategy='most_frequent')),  # Remplit les NaN avec 0\n",
    "    ('scaler', MinMaxScaler())                 # Standardisation\n",
    "])\n",
    "\n",
    "# Encoding pipeline\n",
    "\n",
    "column_transformer =  ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', onehot_pipeline, categorical_columns_onehot),\n",
    "        ('numeric', numeric_pipeline, numerical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def build_xgboost_model():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('encoding', column_transformer),\n",
    "        ('model', XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=10,\n",
    "            subsample = 0.8\n",
    "        ))\n",
    "    ])\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = build_xgboost_model()\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "y_pred = xgb_model.predict(X_train)\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_train, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y_train, y_pred))\n",
    "sns.scatterplot(x=y_train,y=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "sns.scatterplot(x=y_test,y=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_predictions(y_train, y_test, X_train, X_test, model, save_path):\n",
    "    \"\"\"\n",
    "    Génère une figure avec deux graphiques côte à côte : prédictions pour l'ensemble d'entraînement et de test.\n",
    "\n",
    "    Parameters:\n",
    "    - y_train : array-like, les vraies valeurs de l'ensemble d'entraînement.\n",
    "    - y_test : array-like, les vraies valeurs de l'ensemble de test.\n",
    "    - X_train : array-like, les features de l'ensemble d'entraînement.\n",
    "    - X_test : array-like, les features de l'ensemble de test.\n",
    "    - model : object, le modèle entraîné qui possède une méthode .predict().\n",
    "    - save_path : str, chemin pour sauvegarder la figure générée.\n",
    "\n",
    "    Returns:\n",
    "    - None. La figure est sauvegardée à l'emplacement spécifié par save_path.\n",
    "    \"\"\"\n",
    "    # Prédictions pour les ensembles d'entraînement et de test\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Calcul des métriques pour l'ensemble d'entraînement\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "    # Calcul des métriques pour l'ensemble de test\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    # Création de la figure\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Graphique 1 : Ensemble d'entraînement\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x=y_train, y=y_pred_train, alpha=0.6, edgecolor=None)\n",
    "    plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color=\"red\", linestyle=\"--\", label=\"Perfect Prediction\")\n",
    "    plt.title(\"Train Set\", fontsize=14)\n",
    "    plt.xlabel(\"True Values\", fontsize=12)\n",
    "    plt.ylabel(\"Predicted Values\", fontsize=12)\n",
    "    plt.text(\n",
    "        0.05, 0.95,  # Position dans le graphique (proportions)\n",
    "        f\"MSE: {mse_train:.2f}\\nR²: {r2_train:.2f}\",\n",
    "        fontsize=10,\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        bbox=dict(facecolor='white', alpha=0.8, edgecolor='black')\n",
    "    )\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Graphique 2 : Ensemble de test\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(x=y_test, y=y_pred_test, alpha=0.6, edgecolor=None)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\"red\", linestyle=\"--\", label=\"Perfect Prediction\")\n",
    "    plt.title(\"Test Set\", fontsize=14)\n",
    "    plt.xlabel(\"True Values\", fontsize=12)\n",
    "    plt.ylabel(\"Predicted Values\", fontsize=12)\n",
    "    plt.text(\n",
    "        0.05, 0.95,  # Position dans le graphique (proportions)\n",
    "        f\"MSE: {mse_test:.2f}\\nR²: {r2_test:.2f}\",\n",
    "        fontsize=10,\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        bbox=dict(facecolor='white', alpha=0.8, edgecolor='black')\n",
    "    )\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Sauvegarde de la figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()  # Fermer la figure après sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test_predictions(\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    model=xgb_model,\n",
    "    save_path=\"train_test_scatter_plots_with_metrics.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "def save_model(pipeline, filename):\n",
    "    \"\"\"Sauvegarde le modèle dans un fichier\"\"\"\n",
    "    joblib.dump(pipeline, filename)\n",
    "    print(f\"Modèle sauvegardé sous {filename}\")\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"Charge un modèle depuis un fichier\"\"\"\n",
    "    return joblib.load(filename)\n",
    "\n",
    "save_model(xgb_model, 'xgboost_model.pkl')  # Sauvegarde\n",
    "\n",
    "# Pour charger et utiliser le modèle sauvegardé\n",
    "loaded_model = load_model('xgboost_model.pkl')\n",
    "y_pred_2 = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_pred_2, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y_pred_2, y_pred))\n",
    "sns.scatterplot(x=y_pred_2,y=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_xgboost(pipeline, X_train, y_train):\n",
    "    # Définition des paramètres à tester dans la grille\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [100, 300, 500],         # Nombre d'arbres\n",
    "        'model__learning_rate': [0.01, 0.05, 0.1],     # Taux d'apprentissage\n",
    "        'model__max_depth': [5, 10, 15],               # Profondeur maximale\n",
    "        # 'model__subsample': [0.8, 1.0],                # Sous-échantillonnage des données\n",
    "        # 'model__colsample_bytree': [0.8, 1.0],         # Sous-échantillonnage des colonnes\n",
    "        'model__gamma': [0, 1, 5],                     # Régularisation\n",
    "    }\n",
    "\n",
    "    # Configuration de la GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,                  # Nombre de folds pour la validation croisée\n",
    "        scoring='neg_mean_squared_error',  # Métrique d'évaluation\n",
    "        n_jobs=2,\n",
    "        verbose=1              # Affichage des étapes\n",
    "    )\n",
    "\n",
    "    # Exécution de la recherche\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "pipeline = build_xgboost_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbest_model = grid_search_xgboost(pipeline, X_train, y_train)\n",
    "# y_pred = xgbest_model.predict(X_test)\n",
    "# print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "# print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "# sns.scatterplot(x=y_test,y=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df_final = pl.read_csv(\"data_processed/data_dvf_preprocessed.csv\",columns=[\n",
    "                                        'surface_reelle_bati', 'year','type_local', 'sin_month', 'cos_month', 'nombre_lots',\n",
    "                                        'total_surface_carrez', 'lot1_surface_carrez', 'lot2_surface_carrez',\n",
    "                                        'lot3_surface_carrez', 'lot4_surface_carrez', 'lot5_surface_carrez',\n",
    "                                        'nombre_pieces_principales', 'surface_terrain', 'longitude', 'latitude','valeur_fonciere',\n",
    "                                        'densite_weighted', 'transport_pois_weighted', 'education_pois_weighted',\n",
    "                                        'health_pois_weighted', 'food_pois_weighted', 'shopping_pois_weighted',\n",
    "                                        'park_pois_weighted', 'entertainment_pois_weighted', 'cultural_pois_weighted'\n",
    "                                        ],\n",
    "                                    schema_overrides={\n",
    "                                            'surface_reelle_bati':pl.Float32,\n",
    "                                            'type_local':pl.Utf8,\n",
    "                                            'year':pl.Float32,\n",
    "                                            'sin_month':pl.Float32,\n",
    "                                            'cos_month':pl.Float32,\n",
    "                                            'nombre_lots':pl.Float32,\n",
    "                                            'total_surface_carrez':pl.Float32,\n",
    "                                            'lot1_surface_carrez':pl.Float32,\n",
    "                                            'lot2_surface_carrez':pl.Float32,\n",
    "                                            'lot3_surface_carrez':pl.Float32,\n",
    "                                            'lot4_surface_carrez':pl.Float32,\n",
    "                                            'lot5_surface_carrez':pl.Float32,\n",
    "                                            'nombre_pieces_principales':pl.Float32,\n",
    "                                            'surface_terrain':pl.Float32,\n",
    "                                            'longitude':pl.Float32,\n",
    "                                            'latitude':pl.Float32,\n",
    "                                            'valeur_fonciere':pl.Float32,\n",
    "                                            'densite_weighted':pl.Float32,\n",
    "                                            'transport_pois_weighted':pl.Float32,\n",
    "                                            'education_pois_weighted':pl.Float32,\n",
    "                                            'health_pois_weighted':pl.Float32,\n",
    "                                            'food_pois_weighted':pl.Float32,\n",
    "                                            'shopping_pois_weighted':pl.Float32,\n",
    "                                            'park_pois_weighted':pl.Float32,\n",
    "                                            'entertainment_pois_weighted':pl.Float32,\n",
    "                                            'cultural_pois_weighted':pl.Float32\n",
    "                                        },\n",
    "                                    ignore_errors=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Création d'un DataFrame Polars avec NaN dans col_1\n",
    "df = pl.DataFrame({\n",
    "    \"col_1\": [np.nan],\n",
    "    \"col_2\": [3]\n",
    "})\n",
    "\n",
    "# Remplacer NaN dans 'col_1' par 0 avant l'addition\n",
    "df = df.with_columns(\n",
    "    (pl.col(\"col_1\") + pl.col(\"col_2\") > 2).alias(\"result\")\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upfund_dvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
